<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name=viewport content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta name="google-site-verification" content="XXjK99sZKdpFN8kxwbglvDK8Gbhanfx-QfZGGG_TU1M" />
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px
    }

    abstract {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 13px;
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 23px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 33px;
    }

    #paper_img {
      width: 200;
      height: 200;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <title>Purva Tendulkar</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-174206199-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-174206199-1');
  </script>
</head>

<body>
  <table width="815" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Purva Tendulkar</name>
              </p>
              <p>I am a Visiting Research Staff at <a href="https://ucsd.edu/">UC San Diego</a>, where I work with Prof. <a href="https://xiaolonw.github.io/">Xiaolong Wang</a> on computer vision and machine learning. My research focus is on modeling visual temporal dynamics.</p>

              <p>I completed my Master's in Computer Science at <a href="https://www.gatech.edu/">Georgia Tech</a> in 2020 where I was
                advised by Prof. <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>. My Master's Thesis is available <a href="https://smartech.gatech.edu/handle/1853/63699/">here</a>. I have also collaborated with <a href="https://anikem.github.io/">Ani Kembhavi</a> at <a href="https://allenai.org/">Allen AI</a>.
                In 2018, I obtained my Bachelor's Degree in Computer Science from <a
                  href="https://www.coep.org.in/">College of Engineering Pune (COEP), India</a>.
              </p>
              <p> I've had the pleasure of interning at: </p>
              <ul>
                <li><a href="https://aibee.com/en">AiBee</a>, Summer
                  2019 with Prof. <a href="https://cvgl.stanford.edu/silvio/">Silvio Savarese</a>, <a
                    href="https://scholar.google.com/citations?user=Te3H6mMAAAAJ&hl=en">Chunhui Gu</a> and <a
                    href="http://www.niebles.net/">Juan Carlos Niebles</a>. </li>
                <li><a href="https://www.zillow.com/">Nanyang Technological University</a>, Summer 2017 with Prof. <a
                    href="https://www3.ntu.edu.sg/home/arvinde/">Arvind Easwaran</a> and Prof. <a
                    href="https://research.ntu.edu.sg/expertise/academicprofile/Pages/StaffProfile.aspx?ST_EMAILID=ANUPAM">Anupam Chattopadhyay</a></li>
              </ul>

              <!-- <p>I am a <a href="https://snapresearchfs.splashthat.com/"> Snap Research Scholar (2019)</a> and a <a
                  href="https://www.siebelscholars.com/scholar-profile/1425/">Siebel Scholar (2018)</a>!</p> -->
              <p align=center>
                <a href="mailto:purva@gatech.edu">Email</a> &nbsp/&nbsp
                <a href="files/Purva_Tendulkar_Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=-7JsjAsAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/purvaten"> Github </a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/purvaten/"> LinkedIn</a>
              </p>
            </td>
            <td width="50%">
              <img width="100%" src="images/purva.png">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
            </td>
          </tr>
        </table>
        <table id="publications" width="100%" align="center" border="0" border-spacing="0" border-collapse="separate"
          cellspacing="0" cellpadding="20">
          <!-- <tr bgcolor="#ffffd0"> -->
          <tr>
            <td width="25%" valign="top">
              <img id="paper_img" src='images/sort20.jpg'>
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/pdf/2010.10038.pdf">
                <papertitle>SOrT-ing VQA Models : Contrastive Gradient Learning for Improved Consistency
                </papertitle>
              </a>
              <br>
              <a href="https://sameerdharur.github.io/">Sameer Dharur</a>,
              <strong>Purva Tendulkar</strong>,
              <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,
              <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,
              <a href="https://ramprs.github.io/">Ramprasaath R. Selvaraju</a>
              <br>
              <!-- <em>NeurIPS Workshop on Interpretable Inductive Biases and Physically Structured Learning</em>, 2020 -->
              <em>Interpretable Inductive Biases and Physically Structured Learning, NeurIPS</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2010.10038">arXiv</a> /
              <a href="https://youtu.be/LfQ69O9kAcY">talk</a> /
              <a href="https://github.com/sameerdharur/sorting-vqa">code</a> /
              <a href="files/sort_poster.pdf">poster</a> /
              <a href="files/neurips20_bib.txt">bib</a>
              <br>
              <p align="justify">
                <abstract>
                We present a gradient-based interpretability approach to Visual Question Answering (VQA) to determine the questions most strongly correlated with the reasoning question on an image, and use this to evaluate VQA models on their ability to identify the relevant sub-questions needed to answer a reasoning question. Next, we propose a contrastive gradient learning based approach, SOrT, and show improvements in model consistency and visual grounding.
                </abstract>
              </p>
            </td>
          </tr>
          <!-- <tr>
            <td width="25%" valign="top">
              <img id="paper_img" src='images/gt.jpg'>
            </td>
            <td width="75%" valign="center">
              <a href="https://smartech.gatech.edu/handle/1853/63699/">
                <papertitle>Computational methods for creative inspiration in thematic typography and dance</papertitle>
              </a>
              <br>
              <strong>Purva Tendulkar</strong>
              <br>
              <em><i>Masters Thesis</i></em> 2020
              <br>
              <a href="https://smartech.gatech.edu/handle/1853/63699">Dissertation</a>
            </td>
          </tr> -->
          <tr>
            <td width="25%" valign="top">
              <img id="paper_img" src='images/iccc20.jpg'>
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/pdf/2006.11905.pdf">
                <papertitle>Feel The Music: Automatically Generating A Dance For An Input Song</papertitle>
              </a>
              <br>
              <strong>Purva Tendulkar</strong>,
              <a href="https://abhishekdas.com/">Abhishek Das</a>,
              <a href="https://anikem.github.io/">Aniruddha Kembhavi</a>,
              <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>
              <br>
              <em>ICCC</em>, 2020 <font color="red"><strong><i>(Oral)</i></strong></font>
              <br>
              <a href="https://arxiv.org/abs/2006.11905">arXiv</a> /
              <a href="https://youtu.be/2oQuDTvE4To"> talk</a> /
              <a href="images/iccc20.m4v"> video</a> /
              <a href="https://sites.google.com/view/dancing-agents"> dances</a> /
              <a href="https://github.com/purvaten/feel-the-music"> code</a> /
              <a href="https://tech.fb.com/ai-like-the-way-you-move-how-facebook-researchers-built-an-inspirational-dancing-machine/"> Tech@Facebook article</a> /
              <a href="https://www.engadget.com/facebook-ai-choreography-170023727.html"> press</a> /
              <a href="files/iccc20_bib.txt">bib</a>
              <br>
              <p align="justify">
                <abstract>
                We present a general computational approach that enables a machine to generate a dance for any input music. We encode intuitive, flexible heuristics for what a 'good' dance is: the structure of the dance should align with the structure of the music. This flexibility allows the agent to discover creative dances. Human studies show that participants find our dances to be more creative and inspiring compared to meaningful baselines. We also evaluate how perception of creativity changes based on different presentations of the dance.
                </abstract>
              </p>
            </td>
          </tr>
          <tr>
            <td width="25%" valign="top">
              <img id="paper_img" src='images/cvpr20.jpg'>
            </td>
            <td width="75%" valign="center">
              <a href=https://arxiv.org/pdf/2001.06927.pdf">
                <papertitle>SQuINTing at VQA Models: Interrogating VQA Models with Sub-Questions</papertitle>
              </a>
              <br>
              <a href="https://ramprs.github.io/">Ramprasaath R. Selvaraju</a>,
              <strong>Purva Tendulkar</strong>,
              <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,
              <a href="http://erichorvitz.com/">Eric Horvitz</a>,
              <a href="https://homes.cs.washington.edu/~marcotcr/">Marco Tulio Ribeiro</a>,
              <a href="https://besmiranushi.com/">Besmira Nushi</a>,
              <a href="https://www.ecekamar.com/">Ece Kamar</a>
              <br>
              <em>CVPR</em>, 2020 <font color="red"><strong><i>(Oral)</i></strong></font>
              <br>
              <a href="https://arxiv.org/abs/2001.06927">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=v3HhAfj7WpU"> talk</a> /
              <a href="https://www.microsoft.com/en-us/research/project/vqa-introspect/"> data</a> /
              <a href="files/cvpr20_bib.txt"> bib</a>
              <br>
              <p align="justify">
                <abstract>
                We investigate the capabilities of VQA models for solving tasks that differ in nature and in complexity. We notice that existing VQA models have consistency issues -- they answer complex reasoning question correctly but fail on associated low-level perception sub-questions. We quantify the extent to which this phenomenon occurs by creating a new Reasoning split and collecting Sub-VQA, a new dataset consisting of associated perception sub-questions needed to effectively answer the main reasoning question. Additionally, we propose SQuINT approach which enforces models be right for the right reasons.
                </abstract>
              </p>
            </td>
          </tr>
          <tr>
            <td width="25%" valign="top">
              <img id="paper_img" src='images/iccc19.jpg'>
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/pdf/1903.07820.pdf">
                <papertitle>Trick or TReAT: Thematic Reinforcement for Artistic Typography</papertitle>
              </a>
              <br>
              <strong>Purva Tendulkar</strong>,
              <a href="https://martiansideofthemoon.github.io/">Kalpesh Krishna</a>,
              <a href="https://ramprs.github.io/">Ramprasaath R. Selvaraju</a>,
              <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>
              <br>
              <em><i>ICCC</i></em>, 2019 <font color="red"><strong><i>(Oral; Best Presentation Award)</i></strong></font>
              <br>
              <a href="https://arxiv.org/abs/1903.07820">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=c9MvbeQkZwQ"> talk</a> /
              <a href="http://doodle.cloudcv.org/"> demo</a> /
              <a href="https://github.com/purvaten/treat"> code</a> /
              <a href="files/iccc19_bib.txt"> bib</a>
              <br>
              <p align="justify">
                <abstract>
                We present a computational approach for semantic reinforcement called TReAT. Given an input word (e.g. exam) and a theme (e.g. education), the individual letters of the input word are replaced by cliparts relevant to the theme which visually resemble the letters - adding creative context to the potentially boring input word. We use an unsupervised approach to learn a latent space to represent letters and cliparts and compute similarities between the two. Human studies show that participants can reliably recognize the word as well as the theme in our outputs (TReATs) and find them more creative compared to meaningful baselines.
                </abstract>
              </p>
            </td>
          </tr>
        </table>

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="75%" valign="center">
              <p>
              <p style="font-size:18px">I am/have been a teaching assistant for the following courses:</p>
              <ul>
                <li><a href="https://courses.engr.illinois.edu/cs440/fa2018/">
                    <papertitle>Artificial Intelligence (CS440) - Fall 2018</papertitle>
                  </a></li>
                <li><a href="https://cs125.cs.illinois.edu/">
                    <papertitle>Introduction to Computer Science (CS125) - Spring 2018
                  </a>
                </li>
              </ul>
              </p>
            </td>
          </tr>
        </table> -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                  <a href="http://www.cs.berkeley.edu/~barron/">Cloned from here!</a>
                </font>
              </p>
            </td>
          </tr>
        </table>

</body>

</html>